**Session 5 – Monocular Human Pose Estimation and ONNX Models**

<a href="#1">Model Architecture</a>

<a href="#2">Sample Outputs</a> 



<p id="1">**Model Architecture and Joints MSE Class**</p>



**Model Architecture:**

- ResNet is the most common backbone network for image feature extraction. It is also used in for pose estimation. Our method simply adds a few Deconvolutional layers over the last convolution stage in the ResNet, called C5. The whole network structure is illustrated in Figure 1 below:



<p align="center">
  <img width="600" height="600" src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Fig%201.%20Pose%20Estimation.png">
</p>

- We adopt this structure because it is arguably the simplest to generate heat maps from deep and low resolution features and also adopted in the state-of-the-art Mask R-CNN.  
- By default, three deconvolutional layers with batch normalization and ReLU activation are used. Each layer has 256 filters with 4 × 4 kernel. The stride is 2. 
- A 1 × 1 convolutional layer is added at last to generate predicted heat maps { H1, H2, …. Hk} for all k key points.
- Mean Squared Error (MSE) is used as the loss between the predicted heatmaps and targeted heatmaps. The targeted heatmap H^k for joint k is generated by applying a 2D gaussian centered on the kth joint’s ground truth location.


**How it is simple and effective method compared to other architectures? (Hourglass, CPN Vs Simple Baseline)**

- Hourglass is the dominant approach on MPII benchmark as it is the basis for all leading methods . It features in a multi-stage architecture with repeated bottom-up, top-down processing and skip layer feature concatenation.

- Cascaded pyramid network (CPN) [6] is the leading method on COCO 2017 keypoint challenge [9]. It also involves skip layer feature concatenation and an online hard keypoint mining step.

- It is clear that our method differs from in how high resolution feature maps are generated. Both works use up sampling to increase the feature map resolution and put convolutional parameters in other blocks. In contrary, our method combines the up sampling and convolutional parameters into deconvolutional layers in a much simpler way, without using skip layer connections.

- A commonality of the three methods is that three up sampling steps and also three levels of non-linearity (from the deepest feature) are used to obtain high resolution feature maps and heat maps. Based on above observations and the good performance of our baseline, it seems that obtaining high resolution feature maps is crucial, but no matter how.


**Architecture Code:**
**ResNet with Deconvolution Layers:**


<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Fig%202.png">
</p>


<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Fig%203.png">
</p>

**JointsMSELoss Function:**

**Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values**

<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Fig%204.png">
</p>

Mean Squared Error (MSE) is used as the loss between the predicted heatmaps and targeted heatmaps. The targeted heatmap H^k for joint k is generated by applying a 2D gaussian centered on the kth joint’s ground truth location.



<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Fig%205.png">
</p>

**Pose Tracking Based on Optical Flow:**
- Multi-person pose tracking in videos first estimates human poses in frames, and then tracks these human pose by assigning a unique identification number (id)
to them across frames. We present human instance P with id as P = (J, id),  where J = {ji} 1:NJ is the coordinates set of NJ body joints and id indicates the
tracking id. When processing the kth frame Ik, we have the already processed human instances set Pk−1 = {Pik-1} 1:Nk-1 in frame Ik−1 and the instances set Pk = {Pik} 1:Nk in frame Ik whose id is to be assigned.

- The greedy matching algorithm is to first assign the id of Pik-1 in frame ik-1 to Pkj in frame Ik if the similarity between Pik-1 and Pkj is the highest, then remove these two instances from consideration, and repeat the id assigning process with the highest similarity. When an instance Pkj in frame Ik has no existing Pik-1 left to link, a new id number is assigned, which indicates a new instance comes up.





<p id="2">Sample Outputs
</p>

**Output 1: Straight Pose**
<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Uday.jpg">
</p>

**Output 1: Bended Pose**
<p align="center">
  <img src="https://github.com/gudaykiran/EVA-4-Phase-2/blob/master/Session%205/Jagadeesh.jpg">
</p>
